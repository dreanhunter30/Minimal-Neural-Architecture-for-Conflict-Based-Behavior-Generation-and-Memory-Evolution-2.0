{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fca492f",
   "metadata": {},
   "source": [
    "# ðŸ§  NeuroATMv2: Real Third Path Emergence\n",
    "This version tracks true third-path behavior (action = 3), with selective memory and stable context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc878ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Redesigned TinyNN with an extra hidden layer for more reliable context encoding\n",
    "class TinyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Memory-driven agent with selective conflict resolution and real third path\n",
    "class NeuroATMv2:\n",
    "    def __init__(self, memory_limit=100, conflict_threshold=0.01):\n",
    "        self.memory = []\n",
    "        self.memory_limit = memory_limit\n",
    "        self.threshold = conflict_threshold\n",
    "\n",
    "    def act(self, context):\n",
    "        if not self.memory:\n",
    "            return np.random.randint(2)\n",
    "\n",
    "        sims = [np.dot(context, mem[0]) for mem in self.memory]\n",
    "        if len(sims) < 2:\n",
    "            return self.memory[np.argmax(sims)][1]\n",
    "\n",
    "        top = np.argsort(sims)[-2:]\n",
    "        if abs(sims[top[0]] - sims[top[1]]) < self.threshold:\n",
    "            # Conflict detected, spawn new third-path behavior\n",
    "            new_response = 3\n",
    "            self._add_to_memory(context, new_response, initial_score=1)\n",
    "            return new_response\n",
    "        return self.memory[top[0]][1]\n",
    "\n",
    "    def update(self, context, response, success):\n",
    "        for i, (ctx, resp, score) in enumerate(self.memory):\n",
    "            if np.allclose(ctx, context, atol=0.1) and resp == response:\n",
    "                self.memory[i] = (ctx, resp, score + success)\n",
    "                return\n",
    "        self._add_to_memory(context, response, initial_score=success)\n",
    "\n",
    "    def _add_to_memory(self, context, response, initial_score=0):\n",
    "        if len(self.memory) >= self.memory_limit:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((context, response, initial_score))\n",
    "\n",
    "# Generate data near the decision boundary\n",
    "def generate_conflict_data(n=100):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.uniform(-0.1, 0.1, (n, 2))\n",
    "    Y = np.random.randint(0, 2, size=n)\n",
    "    return X, Y\n",
    "\n",
    "# Setup\n",
    "X, Y = generate_conflict_data()\n",
    "model = TinyNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Pretrain for more stability\n",
    "for _ in range(300):\n",
    "    inputs = torch.tensor(X, dtype=torch.float32)\n",
    "    labels = torch.tensor(Y, dtype=torch.long)\n",
    "    output = model(inputs)\n",
    "    loss = loss_fn(output, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Agent simulation\n",
    "agent = NeuroATMv2()\n",
    "log = []\n",
    "conflict_log = []\n",
    "memory_sizes = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    with torch.no_grad():\n",
    "        ctx = model(torch.tensor(X[i], dtype=torch.float32)).numpy()\n",
    "    action = agent.act(ctx)\n",
    "    if action == 3:\n",
    "        conflict_log.append(i)\n",
    "    reward = 1 if action == Y[i] else -1 if action in [0, 1] else 0\n",
    "    agent.update(ctx, action, reward)\n",
    "    log.append(action)\n",
    "    memory_sizes.append(len(agent.memory))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(log, label='Action: 0/1, 3=Third Path', lw=1.5)\n",
    "plt.scatter(conflict_log, [log[i] for i in conflict_log], color='red', label='Third Path Triggered', zorder=5)\n",
    "plt.title('Agent Actions with Explicit Third Path')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Action')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(memory_sizes, label='Memory size over time', color='green')\n",
    "plt.title('Selective Memory Expansion')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Memory Size')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
